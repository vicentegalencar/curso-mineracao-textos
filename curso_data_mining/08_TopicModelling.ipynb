{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_TopicModelling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOoc1tifYa8ntesREfUGIJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ufrpe-ensino/curso-mineracao-textos/blob/master/08_TopicModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wF5g2pqDmEY",
        "colab_type": "text"
      },
      "source": [
        "# Modelagem de Tópicos com LDA\n",
        "\n",
        "## O que é modelagem de tópicos?\n",
        "A modelagem de tópicos pode ser descrita como um método para localizar um grupo de palavras (ou seja, um tópico) de uma coleção de documentos (um \"corpus\") que melhor representa as informações na coleção. Extrair automaticamente o contexto de grandes corpos de dados e dividi-los em tópicos para uma análise mais detalhada é uma das principais aplicações do Processamento de Linguagem Natural.\n",
        "\n",
        "## O que é um tópico?\n",
        "Um tópico é um padrão repetido de termos coocorrentes em um corpus.\n",
        "\n",
        "## O que é *Latent Dirichlet Allocation* (LDA)?\n",
        "\n",
        "O LDA assume que os documentos são produzidos a partir de uma mistura de tópicos. Esses tópicos geram palavras com base em uma distribuição de probabilidade (LDA é um modelo estatístico generativo).\n",
        "\n",
        "Dado um conjunto de dados de documentos, o LDA tenta descobrir quais tópicos criariam esses documentos em primeiro lugar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdqGa0ozEHG3",
        "colab_type": "text"
      },
      "source": [
        "# Importando dados\n",
        "\n",
        "Base de dados de análise de sentimentos em português, contendo 8199 Tweets classificadas como positivo, negativo e neutro. \n",
        "\n",
        "https://minerandodados.com.br/analise-de-sentimentos-twitter-como-fazer/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJdmIwJC33LQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/minerandodados/mdrepo/master/Tweets_Mg.csv'\n",
        "df = pd.read_csv(url, encoding='utf-8')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKiPBdv9GUN_",
        "colab_type": "text"
      },
      "source": [
        "# Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RoxEWUEFL_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word',\n",
        "                             min_df=10,                        # Mínimo de ocorrências da palavra\n",
        "                             lowercase=True,                   # converte para lowercase\n",
        "                             token_pattern='[a-zA-Z0-9]{4,}',  # palavras com pelo menos 3 caracteres\n",
        "                             # max_features=50000,             # número máximo de palavras\n",
        "                            )\n",
        "\n",
        "tokens_cv = vectorizer.fit_transform(df['Text'])\n",
        "tokens_cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwcu-1s0Gwro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build LDA Model\n",
        "lda_model = LatentDirichletAllocation(n_components=10,           # Número de tópicos\n",
        "                                      max_iter=10,               # Número de interações\n",
        "                                      learning_method='online',\n",
        "                                      random_state=100,          \n",
        "                                      batch_size=128,            \n",
        "                                      evaluate_every = -1,       \n",
        "                                      n_jobs = -1,               # Número de CPUs\n",
        "                                     )\n",
        "lda_output = lda_model.fit_transform(tokens_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iuGmzvMG2SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def mostrar_topicos(vectorizer=vectorizer, lda_model='', n_words=20):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_keywords = []\n",
        "    for topic_weights in lda_model.components_:\n",
        "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
        "    return topic_keywords\n",
        "\n",
        "topic_keywords = mostrar_topicos(vectorizer=vectorizer, lda_model=lda_model, n_words=15)\n",
        "\n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords.columns = ['Palavra '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
        "df_topic_keywords.index = ['Tópico '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
        "df_topic_keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4_tOA7GHJ9i",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessamento\n",
        "\n",
        "Tokenização utilizando o [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.html) do NLTK. \n",
        "\n",
        "Tratamento diferenciado de smileys:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwS8mu6QKTHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.iloc[147]['Text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7xfYmthK4Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbT9NroDKoqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.word_tokenize(df.iloc[147]['Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8ptjK2xK7II",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer(strip_handles=False,  # remover 'mentions'\n",
        "                                 preserve_case=False)\n",
        "tweet_tokenizer.tokenize(df.iloc[147]['Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKoKydrNG9PN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tt = TweetTokenizer(strip_handles=True, \n",
        "                    reduce_len=True, \n",
        "                    preserve_case=False)\n",
        "def preprocessamento(text, join=True):\n",
        "    #remove links, pontos, virgulas,ponto e virgulas dos tweets\n",
        "    #coloca tudo em minusculo\n",
        "    text = re.sub(r\"http\\S+\", \"\", text).lower().replace(',','').replace('.','').replace(';','').replace('-','').replace(':','')\n",
        "    if join:\n",
        "      text = ' '.join(tt.tokenize(text))\n",
        "    else:\n",
        "      text = tt.tokenize(text)\n",
        "    return text\n",
        "\n",
        "tokens_nltk = vectorizer.fit_transform(df['Text'].apply(preprocessamento))\n",
        "tokens_nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfBxR_NvHe6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_output = lda_model.fit_transform(tokens_nltk)\n",
        "\n",
        "topic_keywords = mostrar_topicos(vectorizer=vectorizer, lda_model=lda_model, n_words=15)\n",
        "\n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords.columns = ['Palavra '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
        "df_topic_keywords.index = ['Tópico '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
        "df_topic_keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUbdQEvcLaXV",
        "colab_type": "text"
      },
      "source": [
        "# Visualizando com o pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57m5hhL9LmeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GJQZ6trHvkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.sklearn\n",
        "import pyLDAvis\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(lda_model, \n",
        "                                 tokens_nltk, \n",
        "                                 vectorizer, \n",
        "                                 mds='tsne', \n",
        "                                 sort_topics=False)\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZe3N3AZMNwj",
        "colab_type": "text"
      },
      "source": [
        "## Salvando como HTML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImrwuSSALl0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.save_html(panel, 'LDA.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO_sIZedNRGW",
        "colab_type": "text"
      },
      "source": [
        "# Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ciBVD3NXN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import Phrases\n",
        "\n",
        "docs = df['Text'].apply(lambda x: preprocessamento(x, join=False)).values\n",
        "\n",
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "for idx in range(len(docs)):\n",
        "    for token in bigram[docs[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            docs[idx].append(token)\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "# docs = [' '.join(tokens) for tokens in tokens_nltk]\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
        "\n",
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hchDIQ5YNXfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make a index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_ockTXkPRmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.show_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJqyvYNLjDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "\n",
        "panel = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw7RfmH9PItX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}