{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07-CNN-TextClassification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODjmQuMuCNYbVzph9+WWJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ufrpe-ensino/curso-mineracao-textos/blob/master/07_CNN_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCeIJRCGM4s",
        "colab_type": "text"
      },
      "source": [
        "# Classificação de Textos com CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFFknubSGeC_",
        "colab_type": "text"
      },
      "source": [
        "## Carregar Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPMfiTT9y4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = data = pd.read_csv(\"https://raw.githubusercontent.com/ufrpe-ensino/curso-mineracao-textos/master/data/ClothingReviews.csv\")\n",
        "\n",
        "# descartar classes minoritarias\n",
        "major_classes = list(df['Class Name'].value_counts()[0:4].index)\n",
        "majority      = df.loc[df['Class Name'].isin(major_classes),]\n",
        "minority      = df.loc[~df['Class Name'].isin(major_classes),]\n",
        "\n",
        "print('\\n',5*'=', 'major classes: \\n',    majority['Class Name'].value_counts()[0:4])\n",
        "print('\\n',5*'=', 'minority classes: \\n', minority['Class Name'].value_counts()[4:])\n",
        "\n",
        "majority.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZ4Q5LSGmLi",
        "colab_type": "text"
      },
      "source": [
        "## Codificar Labels e Separar treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZFGpNA3-ppf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "_labels = majority['Class Name'].values.reshape((len(majority['Class Name']), 1))\n",
        "X = majority['Review Text']\n",
        "y = encoder.fit_transform(_labels)\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test  = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDQrrr-V_XqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_train[:4], y_train[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNp-bp4REldT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86mnIfM4Gw8o",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizando Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-JGH1BV-bzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train   = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test    = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "\n",
        "print(vocab_size)\n",
        "print(sentences_train.iloc[2])\n",
        "print(X_train[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmbodUTYEu5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxD8N8AeG2bF",
        "colab_type": "text"
      },
      "source": [
        "## Sequence Padding\n",
        "\n",
        "Um problema que temos é que cada sequência de texto tem na maioria dos casos diferentes comprimentos de palavras. Para corrigir isso, você pode usar `pad_sequence()` que simplesmente preenche a sequência de palavras com zeros. \n",
        "\n",
        "Por padrão, ele anexa zeros, mas queremos anexá-los. Normalmente, não importa se você acrescenta ou acrescenta zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McfGfFYo_CUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train[0, :], y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TFkefspHOeB",
        "colab_type": "text"
      },
      "source": [
        "## Arquitetura da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMnG0lEAvLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 300\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpY9AnPhA07I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koIALEmzHabN",
        "colab_type": "text"
      },
      "source": [
        "## Adicionando embeddings pré-treinados\n",
        "\n",
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzOVZ2vzCXQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-D4hRM2CV7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 1 googlenews.word2vec.300d.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRPhGLTDHo8r",
        "colab_type": "text"
      },
      "source": [
        "### GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eqJimlgHrAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHiLHMT6HuKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 1 glove.840B.300d.sst.txt | cut -c-50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyXEVU1H3Ka",
        "colab_type": "text"
      },
      "source": [
        "## Teste: Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkMbEWw6A34u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "    \n",
        "embedding_matrix = create_embedding_matrix('googlenews.word2vec.300d.txt',\n",
        "                      tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYtX9OjZDLXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix[0:3,:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psvpHf34DRwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ2p3j8jDaUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7bDmmx5H9Ft",
        "colab_type": "text"
      },
      "source": [
        "## Teste: Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41qSXd0sDc7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = create_embedding_matrix('glove.840B.300d.sst.txt',\n",
        "                      tokenizer.word_index, embedding_dim)\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2HNzu7fF-Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2S9rGT-JRIW",
        "colab_type": "text"
      },
      "source": [
        "## Métricas detalhadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KADwY35dINUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "preds = model.predict_classes(X_test)\n",
        "true  = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(metrics.classification_report(true, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3YTYJ5Jfnd",
        "colab_type": "text"
      },
      "source": [
        "## Como melhorar?\n",
        "\n",
        "- Hyperparameter tuning\n",
        "- Adicionar mais filtros (convolution layers), por exemplo:\n",
        "\n",
        "```\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "\n",
        "...\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "\n",
        "...\n",
        "```"
      ]
    }
  ]
}